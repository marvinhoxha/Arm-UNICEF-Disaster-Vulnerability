import numpy as np 
import pandas as pd 
import os
import shutil
import re
from tqdm import tqdm
import yaml
from sklearn.model_selection import train_test_split
import logging

logging.basicConfig()
logger = logging.getLogger()
logger.setLevel(logging.INFO)

source_dir = 'Resized_Images'
train_target_dir = 'Data/train/images'
test_target_dir = 'Data/val/images'
yaml_file_path = 'Data/data.yaml'


def read_data():
    train = pd.read_csv("Data/Resized_Train.csv")
    return train

def clean_data(train):  
    cleaned_data = train.dropna(subset=['bbox'])
    return cleaned_data

def split_data(train):
    cleaned_data = clean_data(train)
    unique_image_ids = cleaned_data['image_id'].unique()
    train_image_ids, val_image_ids = train_test_split(unique_image_ids, test_size=0.2, random_state=42)

    # Create train and validation datasets based on the split image_ids
    train_df = cleaned_data[cleaned_data['image_id'].isin(train_image_ids)]
    val_df = cleaned_data[cleaned_data['image_id'].isin(val_image_ids)]
    
    logging.info("Data split into train and validation sets")
    return train_df, val_df

def string_to_list(s):
    s = s.strip('[]')
    return [float(item) for item in s.split(', ')]


def create_label(path, data, image_width, image_height):
    if not os.path.exists(path):
        os.makedirs(path)
        
    unique_images = data['image_id'].unique()
    
    for i,img in zip(tqdm(range(len(unique_images))), unique_images):
        df = data[data['image_id'] == img]
        output_lines = []
        for index, row in df.iterrows():
            category_id = int(row['category_id']) - 1

            bbox = string_to_list(row['bbox'])

            x_center = round((bbox[0] + bbox[2] / 2) / image_width, 4)
            y_center = round((bbox[1] + bbox[3] / 2) / image_height, 4)
            width = round(bbox[2] / image_width, 4)
            height = round(bbox[3] / image_height, 4)


            output_lines.append(f'{category_id} {x_center} {y_center} {width} {height}')

        with open(f"{path}/{img}.txt", 'w') as f:
            for line in output_lines:
                f.write(line + '\n')
                
                
def copy_images(train_df, val_df, source_dir, train_target_dir, val_target_dir):
    os.makedirs(train_target_dir, exist_ok=True)
    os.makedirs(val_target_dir, exist_ok=True)
    
    for img_id in train_df['image_id'].unique():
        source_path = os.path.join(source_dir, f"{img_id}.tif")
        target_path = os.path.join(train_target_dir, f"{img_id}.tif")
        if os.path.exists(source_path):
            shutil.copy(source_path, target_path)
    
    for img_id in val_df['image_id'].unique():
        source_path = os.path.join(source_dir, f"{img_id}.tif")
        target_path = os.path.join(val_target_dir, f"{img_id}.tif")
        if os.path.exists(source_path):
            shutil.copy(source_path, target_path)      
             
    logging.info("Images copied to train and validation directories")        
                
                
def create_yaml(yaml_file_path):
    config = {
    'train': '/home/marvin/Desktop/books/Passau/AILab/Arm-UNICEF-Disaster-Vulnerability/Data/train', 
    'val': '/home/marvin/Desktop/books/Passau/AILab/Arm-UNICEF-Disaster-Vulnerability/Data/val',   
    'nc': 3,       
    'names': ["Thatch", "Tin", "Other"]  
    }

    with open(yaml_file_path, 'w') as yaml_file:
        yaml.dump(config, yaml_file, default_flow_style=False)

    logging.info(f'YAML file created at {yaml_file_path}')
                    
                
def prepare_data():
    """Prepare data for training"""
    train = read_data()
    train_df, val_df = split_data(train)
    copy_images(train_df, val_df, source_dir, train_target_dir, test_target_dir)
    create_label("Data/train/labels", train_df, 512, 512)
    create_label("Data/val/labels", val_df, 512, 512)
    create_yaml(yaml_file_path)


if __name__ == "__main__":
    prepare_data()                
                
                
