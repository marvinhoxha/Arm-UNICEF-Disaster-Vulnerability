import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import re
import matplotlib.patches as patches
from ultralytics import YOLO
import os
import yaml
import json
from tqdm import tqdm
import mlflow
from ultralytics import settings
import torch
import json
from sklearn.model_selection import ParameterSampler
import sys

torch.cuda.empty_cache()
model = YOLO('last500model.pt')
test = pd.read_csv("Data/Unique_Image_IDs_Val.csv")
validation_true_count = pd.read_csv("Data/Validation_Images_True_Count.csv")

# Function to filter results based on custom confidence thresholds
def filter_results(results, custom_conf_thresholds):
    filtered_boxes = []
    for box in results[0].boxes:
        class_id = int(box.cls)
        if box.conf >= custom_conf_thresholds.get(class_id, 0.5):  # Default threshold if class ID not in dict
            filtered_boxes.append(box)
    return filtered_boxes

# Function to get class counts from predictions
def get_classes_count(pred_json: str):
    classes_count = {}
    classes_count[0] = 0
    classes_count[1] = 0
    classes_count[2] = 0
    for j in json.loads(pred_json):
        classes_count[j['class']] += 1
        
    return classes_count  

# Function to generate the submission file
def generate_submission_file(test, model, custom_conf_thresholds):
    torch.cuda.empty_cache()
    submission_df = pd.DataFrame(columns=["image_id", "Target"])
    for index, row in test.iterrows():
        pred = model.predict(f"Data/val/images/{row['image_id']}.tif", iou=0.2, conf=0.1)
        filtered_boxes = filter_results(pred, custom_conf_thresholds)
        
        # Convert the filtered predictions to JSON format and count classes
        try:
            pred_json = json.dumps([{"class": int(box.cls), "conf": float(box.conf)} for box in filtered_boxes])
            pred_json = get_classes_count(pred_json)
        except Exception as e:
            pred_json = {0: 0, 1: 0, 2: 0}
        
        df = pd.DataFrame({
            "image_id": [f"{row['image_id']}_1", f"{row['image_id']}_2", f"{row['image_id']}_3"],
            "Target": [pred_json[0], pred_json[1], pred_json[2]]
        }, columns=["image_id", "Target"])
        
        submission_df = pd.concat([submission_df, df], axis=0)
    
    return submission_df

# Function to evaluate a given configuration using MAE
def evaluate_configuration(config, test, model, validation_true_count):
    submission_df = generate_submission_file(test, model, config)
    
    # Merge the dataframes on image_id
    merged_df = pd.merge(submission_df, validation_true_count, on='image_id', suffixes=('_pred', '_true'))
    
    # Calculate the Mean Absolute Error (MAE)
    mae = (merged_df['Target_pred'] - merged_df['Target_true']).abs().mean()
    return mae

# Define the search space for the thresholds
# param_grid = {
#     0: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7],
#     1: [0.3, 0.4, 0.5, 0.6, 0.7],
#     2: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
# }
param_grid = {
    0: [0.5, 0.5],
    1: [0.7, 0.7],
    2: [0.5, 0.5]
}

# Generate a random sample of the search space
n_iter_search = 1  # Number of random configurations to try
random_search = list(ParameterSampler(param_grid, n_iter=n_iter_search, random_state=42))

# Redirect stdout to suppress the predictions and keep the progress bar on top
original_stdout = sys.stdout
sys.stdout = open(os.devnull, 'w')

# Iterate over the random configurations with a progress bar
best_score = float('inf')
best_config = None

with tqdm(total=n_iter_search, desc="Random Search Progress", leave=True) as pbar:
    for config in random_search:
        score = evaluate_configuration(config, test, model, validation_true_count)
        if score < best_score:
            best_score = score
            best_config = config
        pbar.update(1)

# Restore stdout
sys.stdout = original_stdout

print("Best configuration:", best_config)
print("Best score:", best_score)

# Save the best configuration and score to a file
file_path = 'Data/score_conf.txt'
directory = os.path.dirname(file_path)
if not os.path.exists(directory):
    os.makedirs(directory)

try:
    with open(file_path, 'w') as file:
        file.write(f"Best configuration: {best_config}\n")
        file.write(f"Best score: {best_score}\n")
    print(f'Statements saved to {file_path}')
except Exception as e:
    print(f'Error saving to file: {e}')
